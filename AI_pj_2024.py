# -*- coding: utf-8 -*-
"""ai_pj_2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F0S08ebG2w-TMa5zIe-ip4VHAwnpo6Gv
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip --qq /content/drive/MyDrive/dataset.zip -d dataset

!pip install torch torchvision torchaudio numpy scikit-learn pillow matplotlib tqdm torchsummary adabelief-pytorch

from torchvision import models, transforms
from torch.utils.data import DataLoader, Dataset
import torch
import torch.nn as nn
import torchaudio
import torch.nn.functional as F
from torch.optim.lr_scheduler import MultiStepLR, StepLR
from torchsummary import summary
import argparse
import numpy as np
import random
import os
from PIL import Image
import matplotlib.pyplot as plt
import time
from tqdm import tqdm
from sklearn.metrics import f1_score
import torchaudio
from torch.utils.data import Dataset
from torchvision import transforms
from adabelief_pytorch import AdaBelief as adabelief

# 모델 학습 관련 파라미터 모음 --> 자유롭게 변경하고, 추가해보세요.

class Args():
  data_type = "2d"
  scheduler = "multistep"
  model = "resnet"
  n_class = 3
  epoch = 200
  phase = "train"
  model_path = "./model_weight_2d.pth"

args = Args()

# 데이터 세트 관련 함수 --> 데이터 증강 기법을 적절하게 추가해보세요.
class TimeMasking(object):
    def __init__(self, T=40, max_masks=1):
        self.T = T
        self.max_masks = max_masks

    def __call__(self, spec):
        for _ in range(0, self.max_masks):
            t = random.randrange(0, self.T)
            t0 = random.randrange(0, spec.shape[1] - t)
            spec[:, t0:t0+t] = 0
        return spec

class FrequencyMasking(object):
    def __init__(self, F=30, max_masks=1):
        self.F = F
        self.max_masks = max_masks

    def __call__(self, spec):
        for _ in range(0, self.max_masks):
            f = random.randrange(0, self.F)
            max_f0 = spec.shape[0] - f
            if max_f0 <= 0:
                continue
            f0 = random.randrange(0, max_f0)
            spec[f0:f0+f, :] = 0
        return spec

# 데이터 증강을 포함한 변환 파이프라인 업데이트
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),  # 랜덤으로 좌우 반전
    transforms.RandomRotation(10),      # 랜덤으로 10도 회전
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 색상 조절
    transforms.ToTensor(),
    TimeMasking(T=40, max_masks=1),
    FrequencyMasking(F=30, max_masks=1)
])

class ImageDataset(Dataset):
    def __init__(self, directory, transform=None):
        self.directory = directory
        self.transform = transform
        self.classes = sorted(os.listdir(directory))
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}
        self.samples = []


        for class_name in self.classes:
           class_dir = os.path.join(directory, class_name)
           for image_name in os.listdir(class_dir):
               self.samples.append((os.path.join(class_dir, image_name), self.class_to_idx[class_name]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image_path, label = self.samples[idx]
        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, label

def get_model(name, n_class, pretrained=True):
    if name == 'vgg16':
        model = models.vgg16(pretrained=pretrained)
        num_features = model.classifier[6].in_features
        model.classifier[6] = nn.Linear(num_features, n_class)
    elif name == 'resnet':
        model = models.resnet18(pretrained=pretrained)
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, n_class)
    else:
        raise ValueError(f"Unknown model name: {name}")

    return model


class Simple2DCNN(nn.Module):
    def __init__(self):
        super(Simple2DCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((10, 10))
        self.fc = nn.Linear(256 * 7 * 7, 6)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = self.pool(F.relu(self.conv4(x)))
        x = self.pool(F.relu(self.conv5(x)))
        x = self.adaptive_pool(x)
        x = x.view(-1, 64 * 7 * 7)
        x = self.fc(x)
        return x

# utils에 포함되었던 함수들 + tic toc 추가. tic toc은 수정하지마세요.
def visualize_audio_batch(audio_signals, labels):
    fig, axes = plt.subplots(4, 4, figsize=(10, 5))

    for i, ax in enumerate(axes.flat):
        if i >= 16:  # 16개의 오디오만 표시
            break
        ax.plot(audio_signals[i].t().numpy())  # 오디오 신호 플롯
        ax.set_title(f'Label: {labels[i]}')
        ax.axis('off')

    plt.tight_layout()
    plt.show()

def tic():
    # 현재 시간을 전역 변수에 저장
    global start_time
    start_time = time.time()

def toc():
    # tic()이 호출된 후 경과한 시간을 계산하고 출력
    elapsed_time = time.time() - start_time

    hours = int(elapsed_time // 3600)
    minutes = int((elapsed_time % 3600) // 60)
    seconds = elapsed_time % 60
    print(f"학습에 소요된 시간은 총 : {hours}시간 {minutes}분 {seconds}초 입니다.")

def validation(model, criterion, val_loader, device):
    model.eval() # 평가모드
    val_loss = []
    preds, true_labels = [], []
    correct_predictions = 0
    total_predictions = 0

    # 평가모드의 경우에는 gradient를 초기화하는 부분이 없음 (backward 필요없음. 오직 평가만!)
    with torch.no_grad(): # 이 블록 내에서 그레디언트 계산을 중단하여, 필요하지 않은 메모리 사용을 줄이고 계산 속도 향상.
        for imgs, labels in iter(val_loader):
            imgs = imgs.float().to(device)
            labels = labels.long().to(device)  # 데이터 타입 long으로 변경한 후 device로 올림 (int로 변경하였을 때, error 발생했음)

            pred = model(imgs)

            loss = criterion(pred, labels)

            # pred는 모델이 반환한 예측값. 각 클래스에 대한 확률 또는 점수를 포함하는 텐서. argmax(1)은 각 샘플에 대해 가장 높은 점수를 가진 클래스의 인덱스를 찾아줌.
            # detach()는 현재 계산 그래프로부터 이 텐서를 분리하여, 이후 연산이 그래프에 기록되지 않도록함. 메모리 사용량 줄임
            # cpu()는 cpu로 옮김 (GPU에 있었다면)
            # numpy()는 텐서를 numpy 배열로 변환
            # tolist()는 numpy 배열을 파이썬 리스트로 변환
            preds += pred.argmax(1).detach().cpu().numpy().tolist()

            _, predicted = torch.max(pred, 1)
            total_predictions += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()

            # 실제 라벨도 위와 동일한 과정 진행
            true_labels += labels.detach().cpu().numpy().tolist()

            val_loss.append(loss.item())

        _val_accuracy = 100 * correct_predictions / total_predictions

        _val_loss = np.mean(val_loss)
        # average = 'macro'는 F1점수를 계산할 때, 각 클래스에 대한 F1점수를 동일한 가중치로 평균내어 전체 클래스에 대한 평균 F1점수를 계산.
        # 각 클래스의 샘플 크기와 관계없이 모든 클래스를 동등하게 취급. 이는 클래스 불균형이 있을 때 유용하며, 모든 클래스를 공평하게 평가하고자 할 때 사용.
        # _val_score = f1_score(true_labels, preds, average='macro')

    return _val_loss, _val_accuracy

# train_model 함수 내에서 optimizer를 AdaBelief로 변경
def train_model(model, train_loader, epochs, device, args):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()

    optimizer = AdaBelief(model.parameters(), lr=0.001, eps=1e-16, betas=(0.9, 0.999), weight_decouple=True, rectify=True)

    if args.scheduler == 'multistep':
        scheduler = MultiStepLR(optimizer, [5, 10], gamma=0.1)
    elif args.scheduler == 'steplr':
        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        total_batches = len(train_loader)
        correct_predictions = 0
        total_samples = 0

        for i, (audio_signals, labels) in enumerate(tqdm(train_loader)):
            audio_signals, labels = audio_signals.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(audio_signals)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

        avg_loss = running_loss / total_batches
        accuracy = correct_predictions / total_samples
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")

        # 스케줄러 업데이트
        scheduler.step()

    print('Finished Training')

    # 모델 저장
    torch.save(model.state_dict(), f"./model_weight_{args.data_type}.pth")
    return model

# 학습 관련 함수 정의

def train_model(model, train_loader, epochs, device, args):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()

    optimizer = adabelief(model.parameters(), lr=0.001, eps=1e-16, betas=(0.9, 0.999), weight_decouple=True, rectify=True)

    if args.scheduler == 'multistep':
        scheduler = MultiStepLR(optimizer, [5,10], gamma=0.1)
    elif args.scheduler == 'steplr':
        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        total_batches = len(train_loader)
        correct_predictions = 0
        total_samples = 0

        for i, (audio_signals, labels) in enumerate(tqdm(train_loader)):
            audio_signals, labels = audio_signals.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(audio_signals)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

        avg_loss = running_loss / total_batches
        accuracy = correct_predictions / total_samples
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")

        # 스케줄러 업데이트
        scheduler.step()

    print('Finished Training')

    # 모델 저장
    torch.save(model.state_dict(), f"./model_weight_{args.data_type}.pth")
    return model

if args.data_type == "1d":

    model = Simple1DCNN()


    train_dataset = AudioDataset(directory='dataset/train')
    val_dataset = AudioDataset(directory='dataset/train')

elif args.data_type == "2d":
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        TimeMasking(T=40, max_masks=1),
        FrequencyMasking(F=30, max_masks=1)
    ])

    if args.model == 'vgg16':
        model = get_model(args.model, args.n_class, pretrained=True)
    elif args.model == 'resnet':
        model = get_model(args.model, args.n_class, pretrained=True)
    elif args.model == 'simple':
        model = Simple2DCNN()
    else:
        raise ValueError(f"Unknown model type: {args.model}")


    train_dataset = ImageDataset(directory='dataset/train', transform=transform)
    #val_dataset = ImageDataset(directory='dataset/val', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
#val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
print(f"학습 데이터 수는 {len(train_dataset)}개 입니다.")
#print(f"검증 데이터 수는 {len(val_dataset)}개 입니다.")

# 모델을 GPU 또는 CPU에 할당
model = model.to(device)

if args.data_type == "1d":
    summary(model.cuda(), (1,16000))
elif args.data_type == "2d":
    summary(model.cuda(), (3,224,224))

# 모델의 Complexity는 아래에 나와있는 Total params를 기준으로 측정하겠습니다.

# 학습 시작 및 종료에 걸린 시간을 측정하기 위한 tic - toc

tic()

model = train_model(model, train_loader, epochs=args.epoch, device=device, args=args)

toc()

if args.data_type == "1d":



    test_dataset = AudioDataset(directory='dataset/test')


elif args.data_type == "2d":
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ])

    test_dataset = ImageDataset(directory='dataset/test', transform=transform)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
print(f"테스트 데이터 수는 {len(test_dataset)}개 입니다.")

def evaluate_model(model, test_loader, device, args):
    if args.model_path:
        model.load_state_dict(torch.load(args.model_path, map_location=device))
    model = model.to(device)

    model.eval()

    criterion = nn.CrossEntropyLoss()

    total = 0
    correct = 0
    total_loss = 0.0
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            print(outputs.shape, labels.shape)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total * 100
    f1 = f1_score(all_labels, all_predictions, average='macro')
    print(f'Test Accuracy: {accuracy:.2f}%, Avg Loss: {avg_loss:.4f}, F1 Score: {f1:.4f}')

    return accuracy, avg_loss, f1

accuracy, avg_loss, f1 = evaluate_model(model, test_loader, device=device, args=args)

print(f"테스트 데이터의 f1 score는 {f1}")
